<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Webcam Object Detector (Start/Stop)</title> <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@4.17.0/dist/tf-backend-webgl.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@emailjs/browser@3/dist/email.min.js"></script>

    <style>
        /* Basic styling for layout and appearance */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 20px;
            background-color: #f0f0f0;
            margin: 0;
            color: #333;
        }
        h1, h2 {
            color: #333;
            margin-bottom: 15px;
            text-align: center;
        }
        /* Container for video and canvas */
        .container {
            position: relative;
            width: 640px; /* Default width */
            max-width: 95%; /* Responsive */
            border: 1px solid #ccc;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            background-color: #fff;
            margin-bottom: 15px;
            overflow: hidden; /* Keep canvas inside */
            border-radius: 4px;
        }
        #video {
            display: block; /* Remove extra space below video */
            width: 100%;
            height: auto; /* Maintain aspect ratio */
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 10; /* Draw on top of video */
        }
        /* Controls section styling */
        .controls {
            padding: 10px 15px;
            background-color: #eee;
            border: 1px solid #ccc;
            border-radius: 4px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap; /* Allow wrapping on smaller screens */
            gap: 10px; /* Space between items */
            width: 640px; /* Match container width */
            max-width: 95%;
            box-sizing: border-box; /* Include padding/border in width */
            margin-bottom: 10px; /* Added margin */
        }
        .control-group {
            display: flex;
            align-items: center;
            gap: 5px;
        }
        .controls label { /* Shared style */
            font-size: 0.9em;
            font-weight: bold;
        }
        .controls select { /* Shared style */
            padding: 5px 8px;
            border: 1px solid #ccc;
            border-radius: 3px;
            font-size: 0.9em;
        }
        /* Start/Stop Button Styling */
        #start-stop-btn {
            padding: 10px 20px;
            font-size: 1em;
            font-weight: bold;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s ease, color 0.3s ease;
            width: 640px; /* Match container width */
            max-width: 95%;
            box-sizing: border-box;
            margin-top: 5px; /* Space below controls */
            margin-bottom: 15px;
        }
        #start-stop-btn.start {
            background-color: #28a745; /* Green */
            color: white;
        }
        #start-stop-btn.stop {
            background-color: #dc3545; /* Red */
            color: white;
        }
         #start-stop-btn:disabled {
             background-color: #cccccc;
             color: #666666;
             cursor: not-allowed;
         }

        /* Status message styling */
        #status {
            font-size: 1em;
            font-weight: bold;
            color: #555;
            margin: 15px 0 5px;
            min-height: 1.2em; /* Prevent layout shifts */
            text-align: center;
            width: 100%;
            padding: 8px;
            background-color: #e9e9e9;
            border-radius: 4px;
            max-width: 640px;
            box-sizing: border-box;
        }
        /* Detection status indicator styling */
        #detection-status {
            font-weight: bold;
            padding: 6px 12px;
            border-radius: 4px;
            background-color: #d3d3d3; /* Default: Idle */
            color: #333;
            transition: background-color 0.3s ease, color 0.3s ease;
            font-size: 0.9em;
            text-align: center;
            flex-grow: 1; /* Take available space */
            margin-left: 10px; /* Space from model select */
        }
        #detection-status.human-detected { background-color: #28a745; color: #fff; }
        #detection-status.objects-detected { background-color: #17a2b8; color: #fff; }
        #detection-status.no-objects { background-color: #ffc107; color: #333; }

        /* Email Notification Styling */
        #emailNotification {
            display: none; /* Hidden by default */
            position: fixed; /* Or absolute depending on layout */
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background-color: #28a745; /* Success green */
            color: white;
            padding: 10px 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
            z-index: 1000; /* Ensure it's on top */
            font-size: 0.9em;
        }
         #emailNotification.error {
             background-color: #dc3545; /* Error red */
         }

        /* Snapshot gallery styling */
        #snapshot-gallery {
            margin-top: 20px;
            width: 100%;
            max-width: 680px; /* Slightly wider than video */
            display: flex;
            flex-wrap: wrap;
            gap: 15px; /* Space between snapshots */
            justify-content: center;
            padding: 10px;
            border-radius: 5px;
            background-color: #e9e9e9;
        }
        .snapshot {
            border: 1px solid #ccc;
            padding: 8px;
            background-color: #fff;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border-radius: 4px;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .snapshot img {
            max-width: 150px; /* Limit snapshot preview size */
            height: auto;
            display: block;
            margin-bottom: 8px;
            border: 1px solid #eee;
            border-radius: 3px;
        }
        .snapshot a {
            text-decoration: none;
            background-color: #007bff;
            color: #fff;
            padding: 5px 10px;
            border-radius: 3px;
            font-size: 0.85em;
            cursor: pointer;
            transition: background-color 0.2s ease;
        }
        .snapshot a:hover { background-color: #0056b3; }
        #no-snapshots {
            width: 100%;
            text-align: center;
            color: #666;
            font-style: italic;
        }
        /* Responsive adjustments */
        @media (max-width: 700px) {
            body { padding: 10px; }
            h1 { font-size: 1.5em; }
            h2 { font-size: 1.2em; }
            .container { width: 100%; max-width: 100%; }
            .controls { flex-direction: column; align-items: stretch; max-width: 100%; }
            .control-group { justify-content: space-between; width: 100%; }
            #start-stop-btn { width: 100%; max-width: 100%; } /* Full width on mobile */
            #detection-status { width: 100%; box-sizing: border-box; margin-left: 0; margin-top: 10px; }
            #status { max-width: 100%; font-size: 0.9em; }
            #snapshot-gallery { max-width: 100%; gap: 10px; }
            .snapshot img { max-width: 120px; }
        }
    </style>
</head>
<body>

    <h1>Webcam Object Detector (Start/Stop)</h1>

    <div id="status">Initializing Application...</div>

    <div class="container" id="video-container">
        <video id="video" autoplay playsinline muted></video>
        <canvas id="canvas"></canvas>
    </div>

    <div class="controls" id="main-controls">
         <div class="control-group">
              <label for="model-select">Select Model:</label>
              <select id="model-select" disabled> <option value="yolov8n">YOLOv8n</option>
                  <option value="yolov11n">YOLOv11n</option>
                  </select>
         </div>
         <div id="detection-status">Status: Idle</div>
     </div>

    <button id="start-stop-btn" class="start" disabled>Start Detecting</button> <div id="emailNotification">Email Sent Successfully!</div>

    <h2>Detected Human Snapshots</h2>
    <div id="snapshot-gallery">
        <p id="no-snapshots">No snapshots taken yet.</p>
        </div>

    <script>
        // Initialize EmailJS (if you have a public key)
        emailjs.init("<KEY>"); 

        // --- DOM Element References ---
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusDiv = document.getElementById('status');
        const modelSelect = document.getElementById('model-select');
        const detectionStatusDiv = document.getElementById('detection-status');
        const snapshotGallery = document.getElementById('snapshot-gallery');
        const noSnapshotsMsg = document.getElementById('no-snapshots');
        const videoContainer = document.getElementById('video-container');
        const controlsDiv = document.getElementById('main-controls');
        const startStopBtn = document.getElementById('start-stop-btn'); // New button reference
        const emailNotificationDiv = document.getElementById('emailNotification');


        // --- Model & Detection Parameters ---
        const INPUT_WIDTH = 640;
        const INPUT_HEIGHT = 640;
        const SCORE_THRESHOLD = 0.3;
        const NMS_THRESHOLD = 0.45;
        const MAX_DETECTIONS = 100;
        const HUMAN_CLASS_ID = 4;

        // --- Snapshot & Email Parameters ---
        const SNAPSHOT_MIN_INTERVAL_MS = 1 * 60 * 1000; // 1 minute snapshot interval
        const EMAIL_COOLDOWN_MS = 10 * 60 * 1000; // 10 minutes email cooldown
        const IMGBB_API_KEY = "<KEY>"; // <-- WARNING: Insecure
        const EMAILJS_SERVICE_ID = "<KEY>"; // <-- WARNING: Insecure
        const EMAILJS_TEMPLATE_ID = "<KEY>"; // <-- WARNING: Insecure
        // NOTE: Email recipient is now hardcoded in takeSnapshot function

        // --- State Variables ---
        let currentModel = null;
        let modelName = modelSelect.value;
        let isDetecting = false; // Tracks if a detection frame is currently processing
        let isDetectionRunning = false; // Tracks if the detection loop should be running (controlled by button)
        let rafId = null; // requestAnimationFrame ID
        let lastSnapshotTime = 0;
        let lastDetectedClassesSet = null;
        let lastNotificationTime = 0; // Timestamp of the last email notification sent

        // --- COCO Class Names ---
        const COCO_CLASSES = {
            0: '', 1: '', 2: '', 3: '', 4: 'asiatic rice borer', 5: 'yellow rice borer', 6: 'rice gall midge', 7: '',
            8: 'brown plant hopper', 9: 'white backed plant hopper', 10: '', 11: '', 12: '', 13: '', 14: '', 15: '',
            16: '', 17: '', 18: '', 19: '', 20: '', 21: '', 22: '', 23: '', 24: 'army worm', 25: 'aphids',
            26: '', 27: '', 28: '', 29: '', 30: '', 31: '', 32: '', 33: '', 34: '', 35: '', 36: '',
            37: '', 38: '', 39: '', 40: '', 41: '', 42: '', 43: '', 44: '', 45: '', 46: '', 47: '',
            48: '', 49: '', 50: '', 51: '', 52: '', 53: '', 54: '', 55: '', 56: 'pieris canidia',
            57: '', 58: '', 59: '', 60: '', 61: '', 62: '', 63: '', 64: '', 65: '', 66: '',
            67: '', 68: '', 69: '', 70: '', 71: '', 72: '', 73: '', 74: '', 75: '', 76: '',
            77: '', 78: '', 79: '', 80: '', 81: '', 82: '', 83: '', 84: '', 85: '', 86: '',
            87: '', 88: '', 89: '', 90: '', 91: '', 92: 'scirtothrips dorsalis hood', 93: '',
            94: '', 95: '', 96: '', 97: 'chlumetia transversa', 98: '', 99: '',
            };


        /**
         * Loads a specified TensorFlow.js GraphModel. Does NOT start detection automatically.
         */
        async function loadModel(name) {
            // Stop any ongoing detection first
            if (isDetectionRunning) {
                stopDetectionLoop(); // Cleanly stop the loop
            }
            statusDiv.textContent = `Switching to ${name}...`;
            console.log(`Loading model: ${name}`);
            detectionStatusDiv.textContent = 'Status: Idle';
            detectionStatusDiv.className = 'detection-status';
            isDetecting = false; // Reset processing flag
            lastSnapshotTime = 0;
            lastDetectedClassesSet = null;
            startStopBtn.disabled = true; // Disable button during load
            modelSelect.disabled = true; // Disable select during load

            // Dispose previous model
            if (currentModel) {
                console.log("Disposing previous model...");
                statusDiv.textContent = `Disposing previous model...`;
                await new Promise(resolve => setTimeout(resolve, 50));
                try { currentModel.dispose(); currentModel = null; } catch (e) { console.error("Error disposing model:", e); }
                 ctx.clearRect(0, 0, canvas.width, canvas.height);
            }

            // Initialize TFJS backend
            statusDiv.textContent = `Initializing TFJS backend...`;
            await new Promise(resolve => setTimeout(resolve, 50));
            await tf.ready();
            try {
                await tf.setBackend('webgl'); await tf.ready(); console.log("Using WebGL backend.");
            } catch (e) {
                console.warn("WebGL backend failed, falling back to CPU.", e);
                try { await tf.setBackend('cpu'); await tf.ready(); console.log("Using CPU backend."); }
                catch (cpuError) { console.error("CPU backend also failed:", cpuError); statusDiv.textContent = "TFJS Backend Error."; return false; }
            }

            // Load and warm up the model
            const modelPath = `./${name}_web_model_insect/model.json`;
            console.log(`Model path: ${modelPath}`);
            statusDiv.textContent = `Loading ${name} model...`;

            try {
                currentModel = await tf.loadGraphModel(modelPath);
                console.log(`${name} model loaded successfully.`);
                statusDiv.textContent = `Warming up ${name}...`;
                await new Promise(resolve => setTimeout(resolve, 100));
                console.time(`Warm-up ${name}`);
                tf.tidy(() => {
                    const dummyInput = tf.zeros([1, INPUT_HEIGHT, INPUT_WIDTH, 3], 'float32');
                    const dummyOutput = currentModel.execute(dummyInput);
                    if (Array.isArray(dummyOutput)) { dummyOutput.forEach(t => t.dispose()); } else if (dummyOutput) { dummyOutput.dispose(); }
                });
                console.timeEnd(`Warm-up ${name}`);
                statusDiv.textContent = `${name} model ready. Select 'Start Detecting'.`; // Update status message
                console.log(`${name} model initialized and warmed up.`);
                modelName = name;

                // Enable controls now that model is ready
                startStopBtn.disabled = false;
                modelSelect.disabled = false; // Keep select enabled unless detection is stopped

                // DO NOT start detection automatically here
                // if (video.readyState >= 4) { startDetectionLoop(); } // REMOVED
                // else { console.warn("Webcam stream not ready yet."); video.addEventListener('canplay', startDetectionLoop, { once: true }); } // REMOVED

                return true; // Model loaded successfully

            } catch (error) {
                statusDiv.textContent = `Error loading/warming ${name}. Check console.`;
                console.error(`Failed to load model (${name} from ${modelPath}):`, error);
                currentModel = null;
                startStopBtn.disabled = true; // Keep button disabled on error
                modelSelect.disabled = false; // Allow user to try another model
                return false; // Model loading failed
            }
        }

        /**
         * Sets up the webcam stream.
         */
        async function setupWebcam() {
             return new Promise((resolve, reject) => {
                if (!navigator.mediaDevices?.getUserMedia) { reject('Browser does not support getUserMedia API.'); return; }
                navigator.mediaDevices.getUserMedia({ video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' }, audio: false })
                .then(stream => {
                    video.srcObject = stream;
                    video.addEventListener('loadedmetadata', () => {
                        canvas.width = video.videoWidth; canvas.height = video.videoHeight;
                        videoContainer.style.height = `${video.videoHeight}px`;
                        console.log(`Webcam stream started: ${video.videoWidth}x${video.videoHeight}`); resolve();
                    }, { once: true });
                     video.addEventListener('error', (e) => { reject(`Webcam video error: ${e.message || 'Unknown error'}`); });
                })
                .catch(err => {
                    console.error("getUserMedia error:", err);
                    let errorMsg = `Webcam Access Error: ${err.name}`;
                    if (err.name === 'NotAllowedError') { errorMsg = 'Webcam permission denied.'; }
                    else if (err.name === 'NotFoundError') { errorMsg = 'No webcam found.'; }
                    reject(errorMsg);
                });
            });
        }

        /**
         * Initiates the object detection loop. Called by the Start button.
         */
        function startDetectionLoop() {
            if (!currentModel) {
                console.warn("Cannot start: No model loaded.");
                statusDiv.textContent = "Load a model first.";
                return;
            }
            if (video.paused || video.ended || video.readyState < 4) {
                console.warn("Cannot start: Video stream not ready.");
                statusDiv.textContent = "Waiting for video stream...";
                // Add listener to re-enable start button if video becomes ready later?
                // Or rely on user clicking start again. Let's keep it simple for now.
                return;
            }
            if (isDetectionRunning) {
                console.log("Detection is already running.");
                return;
            }

            console.log("Starting detection loop via button...");
            isDetectionRunning = true; // Set the main running flag
            statusDiv.textContent = `Detecting objects (${modelName})...`;
            detectFrame(); // Start the first frame processing
        }

        /**
         * Stops the object detection loop. Called by the Stop button or other events.
         */
        function stopDetectionLoop() {
            if (!isDetectionRunning && !isDetecting && !rafId) {
                console.log("Detection loop already stopped.");
                return; // Already stopped
            }
            console.log("Stopping detection loop...");
            isDetectionRunning = false; // Clear the main running flag
            if (rafId) {
                cancelAnimationFrame(rafId);
                rafId = null;
            }
            isDetecting = false; // Reset processing flag

            // Update UI
            statusDiv.textContent = "Detection stopped.";
            updateDetectionStatus(null, null, true); // Reset status indicator
            ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear drawings
            startStopBtn.textContent = "Start Detecting";
            startStopBtn.className = 'start';
            modelSelect.disabled = true; // Disable model select when stopped
        }


        /**
         * Processes the raw output tensor from the YOLO model.
         * (Function content remains the same as previous version)
         */
        async function processDetections(predictionsTensor) {
            const finalBoxes = []; const finalScores = []; const finalClassIds = [];
            let tensorsToDispose = [];
            try {
                const transposedPreds = predictionsTensor.transpose([0, 2, 1]); tensorsToDispose.push(transposedPreds);
                const boxes = transposedPreds.slice([0, 0, 0], [-1, -1, 4]); const classes = transposedPreds.slice([0, 0, 4], [-1, -1, 100]); tensorsToDispose.push(boxes, classes);
                const squeezedBoxes = tf.squeeze(boxes); const squeezedClasses = tf.squeeze(classes); tensorsToDispose.push(squeezedBoxes, squeezedClasses);
                const maxScores = tf.max(squeezedClasses, 1); const classIdsTensor = tf.argMax(squeezedClasses, 1); tensorsToDispose.push(maxScores, classIdsTensor);
                const scoresArray = await maxScores.array();
                const filteredIndices = scoresArray.map((score, index) => score > SCORE_THRESHOLD ? index : -1).filter(index => index !== -1);
                if (filteredIndices.length === 0) return { boxes: [], scores: [], classIds: [] };
                const filteredIndicesTensor = tf.tensor1d(filteredIndices, 'int32'); tensorsToDispose.push(filteredIndicesTensor);
                const boxes_filtered = squeezedBoxes.gather(filteredIndicesTensor); const scores_filtered = maxScores.gather(filteredIndicesTensor); const classIds_filtered = classIdsTensor.gather(filteredIndicesTensor); tensorsToDispose.push(boxes_filtered, scores_filtered, classIds_filtered);
                 const selectedIndicesTensor = await tf.image.nonMaxSuppressionAsync(boxes_filtered, scores_filtered, MAX_DETECTIONS, NMS_THRESHOLD, SCORE_THRESHOLD); tensorsToDispose.push(selectedIndicesTensor);
                const selectedIndices = await selectedIndicesTensor.array();
                const boxesData = await boxes_filtered.array(); const scoresData = await scores_filtered.array(); const classIdsData = await classIds_filtered.array();
                for (const index of selectedIndices) {
                    const classId = classIdsData[index]; const score = scoresData[index]; const [cx, cy, w, h] = boxesData[index];
                    const norm_cx = cx / INPUT_WIDTH, norm_cy = cy / INPUT_HEIGHT, norm_w = w / INPUT_WIDTH, norm_h = h / INPUT_HEIGHT;
                    const y1 = norm_cy - norm_h / 2, x1 = norm_cx - norm_w / 2, y2 = norm_cy + norm_h / 2, x2 = norm_cx + norm_w / 2;
                    finalBoxes.push([ Math.max(0, y1), Math.max(0, x1), Math.min(1, y2), Math.min(1, x2) ]);
                    finalScores.push(score); finalClassIds.push(classId);
                }
                return { boxes: finalBoxes, scores: finalScores, classIds: finalClassIds };
            } catch(error) { console.error("Error during detection processing:", error); return { boxes: [], scores: [], classIds: [] }; }
            finally { tf.dispose(tensorsToDispose); }
        }

        /**
         * Helper function to compare two Sets for equality.
         * (Function content remains the same as previous version)
         */
        function areSetsEqual(setA, setB) {
            if (!setA || !setB) return false; if (setA.size !== setB.size) return false;
            for (const item of setA) { if (!setB.has(item)) return false; } return true;
        }

        /**
         * Performs object detection on a single video frame if isDetectionRunning is true.
         */
        async function detectFrame() {
            // --- Pre-detection Checks ---
            // Stop if the loop is manually stopped OR model unloaded OR video stopped
            if (!isDetectionRunning || !currentModel || video.paused || video.ended || video.readyState < 3) {
                console.log("Stopping detection frame processing.");
                stopDetectionLoop(); // Ensure clean stop and UI update
                return;
            }
            // Prevent concurrent execution if the previous frame is still processing
            if (isDetecting) {
                // console.log("Skipping frame, previous detection ongoing.");
                return;
            }

            isDetecting = true; // Set flag to indicate processing has started
            rafId = null; // Clear previous animation frame ID

            let inputTensor; let predictions;

            // --- Inference and Processing ---
            try {
                [inputTensor, predictions] = tf.tidy(() => {
                    const imgTensor = tf.browser.fromPixels(video);
                    const resizedTensor = tf.image.resizeBilinear(imgTensor.toFloat(), [INPUT_HEIGHT, INPUT_WIDTH]);
                    const normalizedTensor = resizedTensor.div(255.0);
                    const batchedInput = normalizedTensor.expandDims(0);
                    const modelOutput = currentModel.execute(batchedInput);
                    return [null, modelOutput]; // Input disposed by tidy
                });

                const rawOutputTensor = Array.isArray(predictions) ? predictions[0] : predictions;
                if (!rawOutputTensor) { console.warn("Model exec returned null."); /* Dispose handled in finally */ return; }

                const { boxes: finalBoxes, scores: finalScores, classIds: finalClassIds } = await processDetections(rawOutputTensor);

                drawBoundingBoxes(finalBoxes, finalScores, finalClassIds);

                // --- Status Update & Snapshot Trigger Logic ---
                const detectedObjects = finalBoxes.length > 0;
                const humanDetected = finalClassIds.includes(HUMAN_CLASS_ID);
                const currentTime = Date.now();
                const currentClassesSet = new Set(finalClassIds);

                updateDetectionStatus(humanDetected, !detectedObjects);

                if (humanDetected) {
                    const timeElapsedSinceLastSnapshot = currentTime - lastSnapshotTime;
                    const classesChangedSinceLastSnapshot = !areSetsEqual(currentClassesSet, lastDetectedClassesSet);

                    if (timeElapsedSinceLastSnapshot >= SNAPSHOT_MIN_INTERVAL_MS) {
                        if (classesChangedSinceLastSnapshot || lastDetectedClassesSet === null) {
                            console.log(`Triggering snapshot. Reason: ${lastDetectedClassesSet === null ? 'Initial' : 'Classes changed'} & time threshold.`);
                            takeSnapshot(finalBoxes, "Human detected"); // Pass boxes and a reason message
                            lastSnapshotTime = currentTime; // Update snapshot time
                            lastDetectedClassesSet = currentClassesSet; // Update detected classes state
                        }
                    }
                }
            } catch (error) {
                console.error("Error during detection frame:", error);
                statusDiv.textContent = "Detection Error.";
                stopDetectionLoop(); // Stop on error
            } finally {
                 if (predictions) { if (Array.isArray(predictions)) predictions.forEach(t => t?.dispose()); else predictions?.dispose(); }
                 isDetecting = false; // Reset processing flag

                 // Schedule the next frame ONLY if the loop should still be running
                 if (isDetectionRunning && currentModel && !video.paused && !video.ended) {
                     rafId = requestAnimationFrame(detectFrame);
                 } else if (isDetectionRunning) {
                     // If we should be running but video stopped, stop the loop cleanly
                     console.log("Video stopped unexpectedly while detection was running.");
                     stopDetectionLoop();
                 }
            }
        }


        /**
         * Draws bounding boxes and labels on the canvas overlay.
         * (Function content remains the same as previous version)
         */
        function drawBoundingBoxes(boxes, scores, classIds) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            if (!boxes || boxes.length === 0) return;
            const colors = ['#00FF00', '#FF0000', '#0000FF', '#FFFF00', '#FF00FF', '#00FFFF', '#FFA500', '#800080'];
            ctx.lineWidth = 2; ctx.font = '14px Arial';
            boxes.forEach((box, i) => {
                const classId = classIds[i]; const className = COCO_CLASSES[classId] || `ID ${classId}`;
                const color = colors[classId % colors.length]; ctx.strokeStyle = color; ctx.fillStyle = color;
                const [y1Norm, x1Norm, y2Norm, x2Norm] = box;
                const x = x1Norm * canvas.width, y = y1Norm * canvas.height;
                const width = (x2Norm - x1Norm) * canvas.width, height = (y2Norm - y1Norm) * canvas.height;
                const validWidth = Math.max(0, width), validHeight = Math.max(0, height);
                ctx.strokeRect(x, y, validWidth, validHeight);
                const scoreText = `${className}: ${(scores[i] * 100).toFixed(1)}%`;
                const textWidth = ctx.measureText(scoreText).width; const textHeight = parseInt(ctx.font, 10);
                ctx.fillRect(x, y - (textHeight + 4), textWidth + 4, textHeight + 4);
                ctx.fillStyle = '#000000'; ctx.fillText(scoreText, x + 2, y - 4);
            });
        }

        /**
         * Updates the detection status indicator element's text and style.
         * (Function content remains the same as previous version)
         */
        function updateDetectionStatus(isHumanDetected, noObjectsDetected, reset = false) {
            if (reset) { detectionStatusDiv.textContent = 'Status: Idle'; detectionStatusDiv.className = 'detection-status'; return; }
            if (isHumanDetected) { detectionStatusDiv.textContent = 'Status: Human Detected!'; detectionStatusDiv.className = 'detection-status human-detected'; }
            else if (!noObjectsDetected) { detectionStatusDiv.textContent = 'Status: Objects Detected (No Human)'; detectionStatusDiv.className = 'detection-status objects-detected'; }
            else { detectionStatusDiv.textContent = 'Status: No Objects Detected'; detectionStatusDiv.className = 'detection-status no-objects'; }
        }

        /**
         * Displays a temporary notification message.
         * (Function content remains the same as previous version)
         */
        function showNotification(message, isError = false) {
            emailNotificationDiv.textContent = message;
            emailNotificationDiv.className = isError ? 'error' : '';
            emailNotificationDiv.style.display = 'block';
            setTimeout(() => { emailNotificationDiv.style.display = 'none'; }, 3000);
        }

        /**
         * Takes a snapshot, adds it to the gallery, uploads it, and sends an email.
         * (Email address is now hardcoded inside)
         */
        async function takeSnapshot(boxesToDraw, reason = "Snapshot taken") {
            console.log("Processing snapshot...");
            if (noSnapshotsMsg) noSnapshotsMsg.style.display = 'none';

            const tempCanvas = document.createElement('canvas');
            tempCanvas.width = video.videoWidth; tempCanvas.height = video.videoHeight;
            const tempCtx = tempCanvas.getContext('2d');

            // 1. Draw video frame and optional boxes
            tempCtx.drawImage(video, 0, 0, tempCanvas.width, tempCanvas.height);
            if (boxesToDraw?.length > 0) {
                tempCtx.strokeStyle = '#FF0000'; tempCtx.lineWidth = 2;
                boxesToDraw.forEach(box => {
                    const [y1Norm, x1Norm, y2Norm, x2Norm] = box;
                    const x = x1Norm * tempCanvas.width, y = y1Norm * tempCanvas.height;
                    const width = (x2Norm - x1Norm) * tempCanvas.width, height = (y2Norm - y1Norm) * tempCanvas.height;
                    tempCtx.strokeRect(x, y, Math.max(0, width), Math.max(0, height));
                });
            }

            // 2. Add to local gallery
            const imageDataUrl = tempCanvas.toDataURL('image/jpeg', 0.9);
            const snapshotDiv = document.createElement('div'); snapshotDiv.classList.add('snapshot');
            const img = document.createElement('img'); img.src = imageDataUrl; img.alt = "Detection snapshot";
            const downloadLink = document.createElement('a'); downloadLink.href = imageDataUrl;
            const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
            downloadLink.download = `snapshot_${timestamp}.jpg`; downloadLink.textContent = 'Save';
            snapshotDiv.appendChild(img); snapshotDiv.appendChild(downloadLink);
            snapshotGallery.insertBefore(snapshotDiv, snapshotGallery.firstChild);
            const maxSnapshots = 20; const snapshotNodes = snapshotGallery.querySelectorAll('.snapshot');
             while (snapshotNodes.length > maxSnapshots && snapshotGallery.lastChild?.classList.contains('snapshot')) {
                 snapshotGallery.removeChild(snapshotGallery.lastChild);
             }

            // 3. --- Email Sending Logic ---
            const currentTime = Date.now();
            if (currentTime - lastNotificationTime < EMAIL_COOLDOWN_MS) {
                console.log(`Email skipped: Cooldown active. (${((EMAIL_COOLDOWN_MS - (currentTime - lastNotificationTime))/1000).toFixed(1)}s remaining)`);
                return; // Exit if within cooldown period
            }

            console.log("Attempting to upload snapshot and send email...");
            // statusDiv.textContent = "Uploading snapshot..."; // Keep main status as 'Detecting...'

            try {
                const imageBlob = await new Promise(resolve => { tempCanvas.toBlob(resolve, 'image/jpeg', 0.7); });
                if (!imageBlob) throw new Error("Failed to create image blob.");

                const formData = new FormData(); formData.append('image', imageBlob);

                // Upload to ImgBB (API Key Warning!)
                const imgbbResponse = await fetch(`https://api.imgbb.com/1/upload?key=${IMGBB_API_KEY}`, { method: 'POST', body: formData });
                const imgbbData = await imgbbResponse.json();

                if (imgbbData.success && imgbbData.data.url) {
                    const imageUrl = imgbbData.data.url;
                    console.log('Image uploaded successfully:', imageUrl);
                    // statusDiv.textContent = "Sending email notification..."; // Keep main status as 'Detecting...'

                    // Send email using EmailJS (Service/Template ID Warning!)
                    await emailjs.send(
                        EMAILJS_SERVICE_ID,
                        EMAILJS_TEMPLATE_ID,
                        {
                            to_email: "antareep2018@gmail.com", // Hardcoded recipient
                            message: `${reason}. Snapshot captured at ${new Date().toLocaleTimeString()}. View image: ${imageUrl}`,
                            image_url: imageUrl
                        }
                    );

                    console.log('Email sent successfully via EmailJS.');
                    showNotification('Email sent with snapshot!');
                    lastNotificationTime = currentTime; // Update last notification time on success
                    // statusDiv.textContent = `Detecting objects (${modelName})...`; // Keep main status

                } else {
                    throw new Error(`ImgBB upload failed: ${imgbbData.error?.message || 'Unknown error'}`);
                }
            } catch (error) {
                console.error('Failed to upload image or send email:', error);
                showNotification(`Error: ${error.message || 'Failed to send snapshot email.'}`, true);
                // statusDiv.textContent = "Email sending failed."; // Keep main status
            }
        }


        // --- Event Listeners Setup ---

        // Handle model selection changes
        modelSelect.addEventListener('change', async (event) => {
            const newModelName = event.target.value;
            if (newModelName !== modelName) {
                console.log(`User selected model: ${newModelName}`);
                // loadModel handles stopping detection and updating UI state
                await loadModel(newModelName);
            }
        });

        // Handle Start/Stop button clicks
        startStopBtn.addEventListener('click', () => {
            if (isDetectionRunning) {
                // --- Stop Detection ---
                stopDetectionLoop();
            } else {
                // --- Start Detection ---
                startDetectionLoop(); // This function handles checks
                // Update button state immediately if start is successful (or handled within startDetectionLoop)
                if (isDetectionRunning) { // Check if startDetectionLoop actually started it
                    startStopBtn.textContent = "Stop Detecting";
                    startStopBtn.className = 'stop';
                    modelSelect.disabled = false; // Enable model select when running
                }
            }
        });

        // Initialize application on window load
        window.addEventListener('load', async () => {
            statusDiv.textContent = "Initializing application...";
            // Controls start disabled

            try {
                statusDiv.textContent = "Requesting Webcam Access...";
                await setupWebcam();
                statusDiv.textContent = "Webcam ready. Loading initial model...";
                // loadModel handles enabling controls on success
                const loaded = await loadModel(modelName);
                if (!loaded) {
                    const errorMsg = `Failed to load initial model: ${modelName}.`;
                    statusDiv.textContent = errorMsg; alert(errorMsg);
                } else {
                     statusDiv.textContent = "Ready. Press 'Start Detecting'."; // Initial ready message
                }
            } catch (error) {
                statusDiv.textContent = `Initialization Error: ${error}`;
                console.error("Initialization failed:", error); alert(`Error initializing: ${error}`);
                // Ensure controls remain disabled on critical init error
                startStopBtn.disabled = true;
                modelSelect.disabled = true;
            }
            // No automatic startDetectionLoop call here
        });

        // Handle page visibility changes
        document.addEventListener('visibilitychange', () => {
            if (!currentModel || !video.srcObject) return; // Only act if initialized

            if (document.hidden) {
                // If detection was running, stop it
                if (isDetectionRunning) {
                    console.log("Page hidden. Stopping detection.");
                    stopDetectionLoop();
                    // Status is updated within stopDetectionLoop
                }
            } else {
                // Page is visible again. DO NOT automatically restart.
                // User must click "Start Detecting" again.
                console.log("Page visible.");
                if (!isDetectionRunning && currentModel) { // If stopped and model exists
                     statusDiv.textContent = "Ready. Press 'Start Detecting'.";
                } else if (isDetectionRunning) {
                    // This case shouldn't happen if stopDetectionLoop works correctly on hide
                    console.warn("Page became visible, but detection loop was unexpectedly running?");
                    statusDiv.textContent = `Detecting objects (${modelName})...`;
                }
            }
        });

    </script>

</body>
</html>
